services:

  # ========================
  # App: master-dataset
  # ========================
  master-dataset:
    build: ../master-dataset
    container_name: master-dataset
    ports:
      - "8082:8080"
    environment:
      HDFS_URI: hdfs://namenode:8020
      LOGGING_FILE_NAME: /app/app.log
    networks:
      - tech-bank-net
    depends_on:
      - namenode

  # ========================
  # App: transaction-processor
  # ========================
  transaction-processor:
    build: ../transaction-processor
    container_name: transaction-processor
    ports:
      - "8083:8080"
      - "4040:4040"   # Spark UI port (default port for job and stage details)      -
    environment:
      HDFS_URI: hdfs://namenode:8020
      LOGGING_FILE_NAME: /app/app.log
    networks:
      - tech-bank-net
    depends_on:
      - namenode
      - spark-master

  # ========================
  # HDFS: namenode
  # ========================
  namenode:
    image: apache/hadoop:3.4.1
    hostname: namenode
    container_name: namenode
    command: >
      bash -c "
      if [ ! -d /tmp/hadoop-hadoop/dfs/name/current ]; then
      hdfs namenode -format -force;
      fi &&
      hdfs namenode
      "
    ports:
      - "9870:9870"
    env_file:
      - ./config
    environment:
      ENSURE_NAMENODE_DIR: "/tmp/hadoop-hadoop/dfs/name"
    volumes:
      - ./data/namenode:/tmp/hadoop-hadoop/dfs/name
    networks:
      - tech-bank-net

  # ========================
  # HDFS: datanode
  # ========================
  datanode:
    image: apache/hadoop:3.4.1
    container_name: datanode
    command: [ "hdfs", "datanode" ]
    env_file:
      - ./config
    depends_on:
      - namenode
    volumes:
      - ./data/datanode:/tmp/hadoop-hadoop/dfs/data
    networks:
      - tech-bank-net

  # ========================
  # Spark: spark-master
  # ========================

  spark-master:
    image: apache/spark:4.0.1-scala2.13-java21-ubuntu
    container_name: spark-master
    command: >
      /opt/spark/bin/spark-class
      org.apache.spark.deploy.master.Master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_DAEMON_JAVA_OPTS=
        -Dspark.serializer=org.apache.spark.serializer.KryoSerializer
        -Dspark.kryo.registrationRequired=false
        --add-opens=java.base/java.lang.invoke=ALL-UNNAMED
        --add-opens=java.base/sun.security.action=ALL-UNNAMED
    volumes:
      - ./spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
    ports:
      - "7077:7077"   # Spark Master RPC
      - "8081:8080"   # Spark Master UI
    networks:
      - tech-bank-net

  # ========================
  # Spark: spark-worker
  # ========================

  spark-worker:
    image: apache/spark:4.0.1-scala2.13-java21-ubuntu
    container_name: spark-worker
    command: >
      /opt/spark/bin/spark-class
      org.apache.spark.deploy.worker.Worker
      spark://spark-master:7077
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_DAEMON_JAVA_OPTS=
        -Dspark.serializer=org.apache.spark.serializer.KryoSerializer
        -Dspark.kryo.registrationRequired=false
        --add-opens=java.base/java.lang.invoke=ALL-UNNAMED
        --add-opens=java.base/sun.security.action=ALL-UNNAMED
    volumes:
      - ./spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
    networks:
      - tech-bank-net
    depends_on:
      - spark-master


networks:
  tech-bank-net:
    driver: bridge